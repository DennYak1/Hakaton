import re
import json
import torch
import numpy as np
import nltk
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM, StoppingCriteria, StoppingCriteriaList
import logging
from nltk.tokenize import sent_tokenize

try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')
try:
    nltk.data.find('tokenizers/punkt_tab')
except LookupError:
    nltk.download('punkt_tab')

logging.basicConfig(level=logging.INFO)

DOCUMENTS_JSON = "output/data.json"
MODEL_NAME = "deepseek-ai/deepseek-coder-7b-instruct-v1.5"

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


class StopOnTokens(StoppingCriteria):
    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:
        stop_ids = [0]
        return input_ids[0][-1] in stop_ids


def split_into_chunks(text, chunk_size=300):
    try:
        sentences = sent_tokenize(text)

    except:
        sentences = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]

    chunks = []
    current_chunk = []
    current_length = 0

    for sent in sentences:
        if current_length + len(sent) > chunk_size and current_chunk:
            chunks.append(" ".join(current_chunk))
            current_chunk = []
            current_length = 0

        current_chunk.append(sent)
        current_length += len(sent)

    if current_chunk:
        chunks.append(" ".join(current_chunk))

    return chunks


class DocumentRetriever:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.retriever = SentenceTransformer(model_name, device='cpu')

    def get_relevant_docs(self, query, documents, top_k=3):
        try:
            query_embedding = self.retriever.encode(query)
            chunks = []

            for doc in documents:
                text = str(doc.get('text', ''))  # –ó–∞—â–∏—Ç–∞ –æ—Ç –¥—ã—Ä–æ–∫
                text_chunks = split_into_chunks(text)

                for i, chunk in enumerate(text_chunks[:5]):
                    chunks.append({
                        'name': f"{doc.get('name', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')} (—á–∞–Ω–∫ {i + 1})",
                        'text': chunk,
                    })

            if not chunks:
                return []

            chunk_embeddings = self.retriever.encode([c['text'] for c in chunks])
            similarities = np.dot(chunk_embeddings, query_embedding)
            top_indices = np.argsort(similarities)[-top_k:][::-1]
            best_chunks = [chunks[i] for i in top_indices]

            return best_chunks if similarities[top_indices[0]] >= 0.25 else []

        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
            return []


def init_llm(model_name):
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True,
            padding_side="left"
        )

        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            trust_remote_code=True,
            torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float16, # –ø–æ—Ç–æ–º –ø–æ–º–µ–Ω—è—Ç—å –Ω–∞ 32
            device_map="auto"
        )
        return tokenizer, model

    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
        raise


def generate_response(query, documents, retriever, tokenizer, model):
    try:
        relevant_docs = retriever.get_relevant_docs(query, documents)

        if not relevant_docs:
            return "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"

        context = format_context(relevant_docs)

        prompt = f"""
        [–ò–ù–°–¢–†–£–ö–¶–ò–ò]
        1. –û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∏–≤–µ–¥–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        2. –ò—Å–ø–æ–ª—å–∑—É–π –ø–æ–Ω—è—Ç–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –∏–∑ —Ä–µ–∞–ª—å–Ω–æ–π –∂–∏–∑–Ω–∏
        3. –ò–∑–±–µ–≥–∞–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
        4. –§–æ—Ä–º–∞—Ç–∏—Ä—É–π –æ—Ç–≤–µ—Ç –∫–∞–∫ –º–∞—Ä–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫

        [–ö–û–ù–¢–ï–ö–°–¢]
        {context}

        [–í–û–ü–†–û–°]
        {query}

        [–û–¢–í–ï–¢]
        """

        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=4096).to(device)

        outputs = model.generate(
            **inputs,
            max_new_tokens=400,
            temperature=0.3,
            top_p=0.85,
            repetition_penalty=1.2,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
            stopping_criteria=StoppingCriteriaList([StopOnTokens()])
        )

        result = tokenizer.decode(outputs[0], skip_special_tokens=True)
        return clean_response(result, query)
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}")
        return "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞"


def format_context(chunks):
    cleaned = []
    for chunk in chunks:
        text = re.sub(r'\.\.\.\d+\.\.\.', '', chunk.get('text', ''))
        text = re.sub(r'\[\d+\.\d+\.\d+\]', '', text)
        cleaned.append(f"üìÑ {chunk.get('name', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')}:\n{text}")
    return "\n\n".join(cleaned)


def clean_response(response, query):
    try:
        if '<|im_start|>assistant' in response:
            response = response.split('<|im_start|>assistant')[-1].strip()

        response = re.sub(r'\[\d+\.\d+\.\d+\]', '', response)

        sentences = sent_tokenize(response)
        return ' '.join(sentences[:10])  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 10 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏ (–ø–æ–∫–∞ —á—Ç–æ) –º–± —Å–¥–µ–ª–∞—Ç—å 2 –≤–µ—Ä—Å–∏–∏
    except:
        return response[:1000]


def main():
    try:
        with open(DOCUMENTS_JSON, 'r', encoding='utf-8') as f:
            documents = json.load(f)
    except FileNotFoundError:
        print(f"–§–∞–π–ª {DOCUMENTS_JSON} –Ω–µ –Ω–∞–π–¥–µ–Ω.")
        return
    except json.JSONDecodeError:
        print(f"–û—à–∏–±–∫–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ —Ñ–∞–π–ª–∞ {DOCUMENTS_JSON}.")
        return
    except Exception as e:
        print(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
        return

    try:
        retriever = DocumentRetriever()
        tokenizer, model = init_llm(MODEL_NAME)
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
        return

    print("\n–î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å!")
    print("–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å. –î–ª—è –≤—ã—Ö–æ–¥–∞ –Ω–∞–∂–º–∏—Ç–µ Enter.")

    while True:
        try:
            user_query = input("\n–í–∞—à –≤–æ–ø—Ä–æ—Å: ").strip()
            if not user_query:
                break

            response = generate_response(
                query=user_query,
                documents=documents,
                retriever=retriever,
                tokenizer=tokenizer,
                model=model
            )

            print("\n–û—Ç–≤–µ—Ç:")
            print(response[:2000])
        except KeyboardInterrupt:
            break
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞: {e}")


if __name__ == "__main__":
    main()